# âœˆï¸ Airline Data Ingestion & Analytics Pipeline (AWS)

## ğŸ“Œ Project Overview

This project demonstrates a **serverless, event-driven airline data ingestion pipeline** built using AWS services.  
The pipeline automatically ingests raw airline datasets, performs ETL transformations, loads them into a data warehouse, and enables analytics using a star schema model.

The solution is designed to simulate a **production-ready data engineering workflow** with orchestration, monitoring, and failure notifications.

---

## ğŸ—ï¸ Architecture Overview

The pipeline follows an event-driven architecture:

- Raw airline data is uploaded to **Amazon S3**
- **Amazon EventBridge** detects the file upload event
- **AWS Step Functions** orchestrates the workflow
- **AWS Glue Crawler** updates the schema in the Data Catalog
- **AWS Glue Job** performs ETL transformations
- Transformed data is loaded into **Amazon Redshift**
- On failure, **Amazon SNS** sends notification alerts

---

## âš™ï¸ AWS Services Used

- **Amazon S3** â€“ Data lake storage  
- **Amazon EventBridge** â€“ Event trigger mechanism  
- **AWS Step Functions** â€“ Workflow orchestration  
- **AWS Glue (Crawler + Job)** â€“ Schema discovery & ETL  
- **Amazon Redshift** â€“ Data warehouse  
- **Amazon SNS** â€“ Failure notifications  

---

## ğŸ”„ Workflow Execution

### 1ï¸âƒ£ Data Ingestion
Raw airline CSV files (dimension & fact data) are uploaded into S3.

### 2ï¸âƒ£ Event Trigger
EventBridge automatically triggers the Step Function workflow when a new file is added.

### 3ï¸âƒ£ Schema Discovery
Glue Crawler scans S3 and updates the Glue Data Catalog.

### 4ï¸âƒ£ ETL Processing
Glue Job:
- Cleans data  
- Handles NULL values  
- Applies column mapping  
- Transforms datasets  
- Writes processed data  

### 5ï¸âƒ£ Data Warehouse Load
Processed data is loaded into Amazon Redshift using a star schema:

- `airport_dim` (Dimension Table)
- `flight_fact` (Fact Table)

### 6ï¸âƒ£ Monitoring & Alerts
If the Glue job fails, SNS sends email notifications.

---

## ğŸ“Š Data Model (Star Schema)

### ğŸ”¹ Dimension Table: `airport_dim`

- airport_id  
- airport_name  
- city  
- state  

### ğŸ”¹ Fact Table: `flight_fact`

- flight_id  
- departure_airport_id  
- arrival_airport_id  
- delay  
- flight_date  

---

## ğŸ“ Project Structure

airline-data-ingestion-pipeline/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ architecture/ # Architecture diagrams & design explanation
â”œâ”€â”€ glue/ # ETL scripts & crawler configs
â”œâ”€â”€ step-functions/ # State machine definitions
â”œâ”€â”€ eventbridge/ # Event rule configuration
â”œâ”€â”€ redshift/ # SQL scripts for schema & queries
â”œâ”€â”€ sns/ # Notification setup
â”œâ”€â”€ sample-data/ # Sample CSV datasets
â”œâ”€â”€ outputs/ # Execution outputs & results
â””â”€â”€ docs/ # Setup guide & troubleshooting


---

## ğŸš€ How to Deploy

1. Create S3 bucket (raw + processed zones)  
2. Upload sample airline data  
3. Configure Glue Crawler  
4. Create Glue ETL Job  
5. Create Redshift tables  
6. Deploy Step Function state machine  
7. Configure EventBridge rule  
8. Configure SNS email subscription  
9. Upload new file to S3 to trigger pipeline  

---

## ğŸ” Key Features

- Event-driven architecture  
- Serverless design  
- Automated schema discovery  
- Orchestrated ETL pipeline  
- Star schema modeling  
- Failure handling with alerts  
- Scalable and production-ready design  

---

## ğŸ“ˆ Sample Output

After successful execution:

- Transformed datasets stored in S3  
- Data loaded into Redshift  
- Step Function execution marked as **SUCCEEDED**  
- Analytics queries can be executed on fact & dimension tables  

---

## ğŸ› ï¸ Future Improvements

- Add data validation framework  
- Implement incremental loading logic  
- Add CloudWatch monitoring dashboards  
- Use Terraform for Infrastructure as Code  
- Add CI/CD pipeline for deployment  
- Implement partitioning strategy in S3  

---

